{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOekyK-SS9v8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "MAX_WORDS_FOR_PREDICTION = 4\n",
        "MAX_WORDS_TO_OUTPUT = 10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm *.txt\n",
        "!wget 'https://raw.githubusercontent.com/r0ckYr/GenerativeAI/main/game_of_thrones.txt'\n",
        "!wget 'https://raw.githubusercontent.com/r0ckYr/GenerativeAI/main/text.txt'\n",
        "!cat text.txt game_of_thrones.txt >> input.txt"
      ],
      "metadata": {
        "id": "h5KUcdE60WfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!du -sh *"
      ],
      "metadata": {
        "id": "LzxKElndcPOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r') as f:\n",
        "  input = f.read()"
      ],
      "metadata": {
        "id": "CQszCJWTZnK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = input.split('\\n')"
      ],
      "metadata": {
        "id": "9rKzy3gt0jJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "random.shuffle(input)"
      ],
      "metadata": {
        "id": "bi6sn2vxbdCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input[:1]"
      ],
      "metadata": {
        "id": "DOfN32jy3lTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = input[:100]"
      ],
      "metadata": {
        "id": "1-imCcyqAxyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(input)\n",
        "# saving the tokenizer for predict function\n",
        "pickle.dump(tokenizer, open('token.pkl', 'wb'))\n",
        "vocab_length = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "BRGHpuvWZz4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "y = []\n",
        "max_sequence_len = 0\n",
        "print(\"Unique words : \", vocab_length)\n",
        "for name in input:\n",
        "  if len(name) > 1:\n",
        "    tokens = tokenizer.texts_to_sequences([name])[0]\n",
        "    tokens_length = len(tokens)\n",
        "    if tokens_length > max_sequence_len:\n",
        "      max_sequence_len = tokens_length\n",
        "\n",
        "    for j in range(0,tokens_length-2):\n",
        "      for i in range(j+1, tokens_length):\n",
        "        X.append(tokens[j:i])\n",
        "        y.append(tokens[i])\n",
        "\n",
        "\n",
        "for i in range(50):\n",
        "  print(X[i], y[i])"
      ],
      "metadata": {
        "id": "i-_74hMleU-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(pad_sequences(X, maxlen=max_sequence_len, padding='pre'))\n",
        "print(X[50])"
      ],
      "metadata": {
        "id": "8EMxmNOViD6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = to_categorical(y, num_classes=vocab_length)\n",
        "y[1]\n",
        "\n"
      ],
      "metadata": {
        "id": "q2KXkQWUiZhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(max_sequence_len)"
      ],
      "metadata": {
        "id": "3nsA0X7kqgv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(X)"
      ],
      "metadata": {
        "id": "xwKdP3qir_4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_length, 100, input_length=max_sequence_len))\n",
        "model.add(Bidirectional(LSTM(1000)))\n",
        "model.add(Dense(vocab_length, activation='softmax'))"
      ],
      "metadata": {
        "id": "QvjJ9pJbjWmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "plot_model(model, to_file=\"plot.png\", show_layer_names=True)"
      ],
      "metadata": {
        "id": "aby6CgNsmoQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint_file = \"word_names_model1.h5\"\n",
        "checkpoint = ModelCheckpoint(checkpoint_file, monitor='loss', verbose=1, save_best_only=True)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=20, batch_size=4, callbacks=[checkpoint], verbose=1)"
      ],
      "metadata": {
        "id": "LYkxntPenDT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = load_model('word_names_model1.h5')\n",
        "tokenizer = pickle.load(open('token.pkl', 'rb'))"
      ],
      "metadata": {
        "id": "2CJ2E8JDyRfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_last_words(input_string):\n",
        "    words = input_string.split()\n",
        "    if len(words) > 20:\n",
        "        return ' '.join(words[-20:])\n",
        "    else:\n",
        "        return input_string"
      ],
      "metadata": {
        "id": "8fcb2tGzQbsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_word(model, tokenizer, text):\n",
        "  # text = extract_last_words(text)\n",
        "  sequence = tokenizer.texts_to_sequences([text])\n",
        "  sequence = np.array(sequence)\n",
        "  preds = np.argmax(model.predict(sequence, verbose=0))\n",
        "  for key, value in tokenizer.word_index.items():\n",
        "      if value == preds:\n",
        "        predicted_word = key\n",
        "  return predicted_word"
      ],
      "metadata": {
        "id": "PwKe2Tahyi9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The morning had dawned\"\n",
        "MAX_WORDS_FOR_PREDICTION = 1000\n",
        "print(text, end=' ')\n",
        "for i in range(MAX_WORDS_FOR_PREDICTION):\n",
        "  next_word = predict_word(model, tokenizer, text)\n",
        "  text = text + \" \" + next_word\n",
        "  if i % 30 == 0 and i!=0:\n",
        "    print()\n",
        "  print(next_word, end = ' ')"
      ],
      "metadata": {
        "id": "Po_hIOz9zC3k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}