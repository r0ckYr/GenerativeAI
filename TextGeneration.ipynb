{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amP3I08TuDNN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://raw.githubusercontent.com/r0ckYr/GenerativeAI/main/text.txt'"
      ],
      "metadata": {
        "id": "_IavsLe5uJ6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('text.txt', 'r') as f:\n",
        "  input = f.read()"
      ],
      "metadata": {
        "id": "SLDZDvmnuLse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = input[:1000]"
      ],
      "metadata": {
        "id": "IaoyIBNYuMNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "bUDgV1dAxKyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Sample vocabulary and dataset\n",
        "vocab = ['<PAD>', 'hello', 'world', 'how', 'are', 'you']\n",
        "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "sequences = [\n",
        "    [1, 2, 3, 4],\n",
        "    [2, 3, 4, 5],\n",
        "    [3, 4, 5, 0]  # Adding padding token <PAD> at the end\n",
        "]\n",
        "\n",
        "# GRU Model\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input)\n",
        "        output, hidden = self.gru(embedded.unsqueeze(1), hidden)  # Adding unsqueeze to match sequence length\n",
        "        output = self.fc(output)\n",
        "        return output, hidden\n",
        "\n",
        "# Training\n",
        "def train_model(model, sequences, word_to_idx, idx_to_word, num_epochs=10, learning_rate=0.01):\n",
        "    vocab_size = len(word_to_idx)\n",
        "    embedding_dim = 10\n",
        "    hidden_dim = 20\n",
        "\n",
        "    model = model(vocab_size, embedding_dim, hidden_dim).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for sequence in sequences:\n",
        "            input_seq = torch.tensor(sequence[:-1], dtype=torch.long, device=device)\n",
        "            target_seq = torch.tensor(sequence[1:], dtype=torch.long, device=device)\n",
        "\n",
        "            hidden = None\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output, hidden = model(input_seq, hidden)\n",
        "            loss = criterion(output.view(-1, vocab_size), target_seq)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = GRUModel\n",
        "    train_model(model, sequences, word_to_idx, idx_to_word, num_epochs=10, learning_rate=0.01)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdTC_W52uOXa",
        "outputId": "42d18d81-d642-49f0-816f-9c40cde6003a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 5.5728\n",
            "Epoch [2/10], Loss: 4.9335\n",
            "Epoch [3/10], Loss: 4.3446\n",
            "Epoch [4/10], Loss: 3.6880\n",
            "Epoch [5/10], Loss: 2.9815\n",
            "Epoch [6/10], Loss: 2.3113\n",
            "Epoch [7/10], Loss: 1.7484\n",
            "Epoch [8/10], Loss: 1.2927\n",
            "Epoch [9/10], Loss: 0.9247\n",
            "Epoch [10/10], Loss: 0.6444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_text(model, start_word, word_to_idx, idx_to_word, max_length=20):\n",
        "    device = next(model.parameters()).device  # Get the device from the model parameters\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input_word = torch.tensor([word_to_idx[start_word]], dtype=torch.long, device=device)\n",
        "        hidden = None\n",
        "        generated_text = [start_word]\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            input_word = input_word.to(device)  # Move input tensor to the correct device\n",
        "            output, hidden = model(input_word, hidden)\n",
        "            output_probs = F.softmax(output, dim=2).squeeze().cpu().numpy()\n",
        "            next_word_idx = np.random.choice(len(output_probs), p=output_probs)\n",
        "            input_word.fill_(next_word_idx)\n",
        "            generated_text.append(idx_to_word[next_word_idx])\n",
        "\n",
        "            if idx_to_word[next_word_idx] == '<PAD>':\n",
        "                break\n",
        "\n",
        "        return ' '.join(generated_text)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = GRUModel(len(vocab), embedding_dim=10, hidden_dim=20)\n",
        "    generated_text = generate_text(model, start_word='world', word_to_idx=word_to_idx, idx_to_word=idx_to_word)\n",
        "    print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSb0YUYSw6v2",
        "outputId": "9c896158-6635-419b-e252-bf35401a0734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "world hello <PAD>\n"
          ]
        }
      ]
    }
  ]
}